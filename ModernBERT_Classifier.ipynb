{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModernBERT Emotion Classifier Tutorial\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to fine-tune the **ModernBERT** model for emotion classification. ModernBERT is a state-of-the-art encoder-only transformer model designed for efficient and accurate sequence classification tasks.\n",
    "\n",
    "### What You'll Learn:\n",
    "- How to load and preprocess the DAIR-AI emotion dataset\n",
    "- How to tokenize text data for BERT-based models\n",
    "- How to fine-tune ModernBERT for multi-class classification\n",
    "- How to evaluate model performance with confusion matrices\n",
    "- How to save and deploy your trained model\n",
    "\n",
    "### Dataset:\n",
    "We'll use the **DAIR-AI Emotion Dataset** which contains 6 emotion categories:\n",
    "- \ud83d\ude22 Sadness\n",
    "- \ud83d\ude0a Joy\n",
    "- \u2764\ufe0f Love\n",
    "- \ud83d\ude20 Anger\n",
    "- \ud83d\ude28 Fear\n",
    "- \ud83d\ude32 Surprise\n",
    "\n",
    "### Model Architecture:\n",
    "**ModernBERT-base** is used as the base model with a classification head added on top for sequence classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "First, we need to install the necessary Python packages:\n",
    "- **datasets**: HuggingFace library for loading and processing datasets\n",
    "- **transformers**: HuggingFace library containing ModernBERT and other transformer models\n",
    "\n",
    "The `-q` flag makes the installation quiet (less verbose output), and `-U` updates to the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install HuggingFace datasets library for loading emotion dataset\n",
    "!pip install -q datasets\n",
    "\n",
    "# Install/upgrade transformers library for ModernBERT model\n",
    "!pip install -Uq transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Tokenizer and Dataset\n",
    "\n",
    "### Tokenizer\n",
    "The tokenizer converts raw text into token IDs that the model can process. ModernBERT uses a WordPiece tokenizer similar to BERT.\n",
    "\n",
    "### Dataset\n",
    "We load the DAIR-AI emotion dataset which contains:\n",
    "- **Training set**: 16,000 examples\n",
    "- **Validation set**: 2,000 examples  \n",
    "- **Test set**: 2,000 examples\n",
    "\n",
    "Each example contains text and a label (0-5) representing one of the six emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the ModernBERT tokenizer\n",
    "# The tokenizer converts text into numerical tokens that the model understands\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the DAIR-AI emotion dataset\n",
    "# This dataset contains tweets/text labeled with 6 different emotions\n",
    "ds = load_dataset(\"dair-ai/emotion\")\n",
    "train_ds = ds[\"train\"]      # Training data (16,000 samples)\n",
    "val_ds = ds[\"validation\"]   # Validation data (2,000 samples)\n",
    "test_ds = ds[\"test\"]        # Test data (2,000 samples)\n",
    "\n",
    "print(f\"Training examples: {len(train_ds)}\")\n",
    "print(f\"Validation examples: {len(val_ds)}\")\n",
    "print(f\"Test examples: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenize the Dataset\n",
    "\n",
    "Tokenization converts text into numerical format:\n",
    "- **padding='max_length'**: Ensures all sequences are the same length\n",
    "- **truncation=True**: Cuts off text longer than max_length\n",
    "- **max_length=300**: Maximum sequence length (chosen based on data analysis)\n",
    "\n",
    "The tokenizer returns:\n",
    "- **input_ids**: Token IDs representing the text\n",
    "- **attention_mask**: 1 for real tokens, 0 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenization function\n",
    "def tokenization(item):\n",
    "    \"\"\"\n",
    "    Tokenize text data with the following parameters:\n",
    "    - padding='max_length': Pad all sequences to max_length\n",
    "    - truncation=True: Truncate sequences longer than max_length\n",
    "    - max_length=300: Maximum sequence length (chosen based on dataset analysis)\n",
    "    \n",
    "    Returns: Dictionary with 'input_ids' and 'attention_mask'\n",
    "    \"\"\"\n",
    "    return tokenizer(item['text'], padding=\"max_length\", truncation=True, max_length=300)\n",
    "\n",
    "# Apply tokenization to training and validation datasets\n",
    "# batched=True processes multiple examples at once for efficiency\n",
    "print(\"Tokenizing training dataset...\")\n",
    "train_ds = train_ds.map(tokenization, batched=True)\n",
    "\n",
    "print(\"Tokenizing validation dataset...\")\n",
    "val_ds = val_ds.map(tokenization, batched=True)\n",
    "\n",
    "print(\"\u2713 Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Data for Training\n",
    "\n",
    "We need to:\n",
    "1. Convert datasets to PyTorch format\n",
    "2. Create DataLoaders for batching\n",
    "3. Define label mappings for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to PyTorch format\n",
    "# This selects only the columns needed for training and converts them to PyTorch tensors\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Define label mapping for better interpretability\n",
    "# Each number (0-5) corresponds to an emotion\n",
    "label_mapping = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\"\n",
    "}\n",
    "\n",
    "# Create DataLoader for batching during training\n",
    "# batch_size=32: Process 32 examples at a time\n",
    "# shuffle=True: Randomize the order of examples (important for training)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"\u2713 Data preparation complete!\")\n",
    "print(f\"  - Training batches: {len(train_dataloader)}\")\n",
    "print(f\"  - Batch size: 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Training\n",
    "\n",
    "### Training Configuration\n",
    "- **Model**: ModernBERT-base with a classification head (6 output classes)\n",
    "- **Optimizer**: AdamW (Adam with weight decay for regularization)\n",
    "- **Learning Rate**: 1e-4 (0.0001)\n",
    "- **Weight Decay**: 1e-2 (0.01) for L2 regularization\n",
    "- **Loss Function**: CrossEntropyLoss (standard for multi-class classification)\n",
    "- **Epochs**: 5 (number of complete passes through the training data)\n",
    "- **Batch Size**: 32\n",
    "\n",
    "### Training Process:\n",
    "1. Load the pre-trained ModernBERT model and add a classification head\n",
    "2. Move the model to GPU (CUDA) for faster training\n",
    "3. For each epoch, iterate through all training batches\n",
    "4. Forward pass: compute predictions\n",
    "5. Backward pass: calculate gradients\n",
    "6. Update model weights using the optimizer\n",
    "7. Track and display average loss per epoch\n",
    "\n",
    "**Note**: The model starts with random weights in the classification head, which will be trained to predict emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import ModernBertForSequenceClassification\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Set up CUDA device for GPU acceleration\n",
    "cuda_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {cuda_device}\")\n",
    "\n",
    "# Load pre-trained ModernBERT model with a classification head\n",
    "# num_labels=6: Output layer has 6 neurons (one for each emotion)\n",
    "model = ModernBertForSequenceClassification.from_pretrained(\n",
    "    \"answerdotai/ModernBERT-base\", \n",
    "    num_labels=6\n",
    ").to(cuda_device)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-4      # Step size for gradient descent\n",
    "weight_decay = 1e-2       # L2 regularization strength (prevents overfitting)\n",
    "num_epochs = 5            # Number of complete passes through training data\n",
    "\n",
    "# Set model to training mode\n",
    "# This enables dropout and batch normalization training behavior\n",
    "model.train()\n",
    "\n",
    "# Initialize optimizer (AdamW is Adam with weight decay)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Define loss function for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Iterate through all batches in the training data\n",
    "    for _, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        # Move batch data to GPU\n",
    "        label = batch['label'].to(cuda_device)              # True emotion labels\n",
    "        input_ids = batch['input_ids'].to(cuda_device)      # Tokenized text\n",
    "        attention_mask = batch['attention_mask'].to(cuda_device)  # Mask for padding tokens\n",
    "        \n",
    "        # Zero out gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute model predictions\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            labels=label\n",
    "        )\n",
    "        \n",
    "        # Extract loss (automatically computed by the model when labels are provided)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model weights based on gradients\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate and display average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\u2713 Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation - Confusion Matrix\n",
    "\n",
    "### What is a Confusion Matrix?\n",
    "A confusion matrix visualizes the performance of a classification model by showing:\n",
    "- **Rows**: True labels (actual emotions)\n",
    "- **Columns**: Predicted labels (what the model predicted)\n",
    "- **Cell values**: Normalized probabilities (what % of each true emotion was predicted as each class)\n",
    "\n",
    "### How to Read the Matrix:\n",
    "- **Diagonal values** (top-left to bottom-right): Correct predictions\n",
    "- **Off-diagonal values**: Misclassifications\n",
    "- **Perfect model**: Would have 1.0 on the diagonal and 0.0 everywhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Create validation DataLoader\n",
    "val_dataloader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize confusion matrix (6x6 for 6 emotions)\n",
    "confusion = torch.zeros(6, 6)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "# This disables dropout and uses batch norm in eval mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Evaluating model on validation set...\")\n",
    "\n",
    "# Disable gradient computation for evaluation (saves memory and computation)\n",
    "with torch.no_grad():\n",
    "    # Iterate through all validation batches\n",
    "    for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "        # Move batch data to GPU\n",
    "        label = batch['label'].to(cuda_device)\n",
    "        input_ids = batch['input_ids'].to(cuda_device)\n",
    "        attention_mask = batch['attention_mask'].to(cuda_device)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get the class with highest probability (argmax)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        # For each (true_label, predicted_label) pair, increment the corresponding cell\n",
    "        for t, p in zip(label.view(-1), preds.view(-1)):\n",
    "            confusion[t.long(), p.long()] += 1\n",
    "\n",
    "# Normalize confusion matrix by row\n",
    "# Each row will sum to 1.0, showing the distribution of predictions for each true class\n",
    "for i in range(6):\n",
    "    denom = confusion[i].sum()\n",
    "    if denom > 0:\n",
    "        confusion[i] = confusion[i] / denom\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Display confusion matrix as a heatmap\n",
    "cax = ax.matshow(confusion.cpu().numpy(), cmap='Blues')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Define emotion labels for axes\n",
    "label_list = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xticks(np.arange(6))\n",
    "ax.set_yticks(np.arange(6))\n",
    "ax.set_xticklabels(label_list, rotation=90)\n",
    "ax.set_yticklabels(label_list)\n",
    "ax.set_xlabel('Predicted Emotion', fontsize=12)\n",
    "ax.set_ylabel('True Emotion', fontsize=12)\n",
    "ax.set_title('Confusion Matrix - Emotion Classification', fontsize=14, pad=20)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Confusion matrix created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Calculate Accuracy\n",
    "\n",
    "### Accuracy Metric\n",
    "Accuracy is the simplest evaluation metric for classification:\n",
    "\n",
    "**Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)**\n",
    "\n",
    "**Expected Result**: A well-trained emotion classifier should achieve 90%+ accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    # Iterate through all validation batches\n",
    "    for batch in val_dataloader:\n",
    "        # Move batch data to GPU\n",
    "        input_ids = batch['input_ids'].to(cuda_device)\n",
    "        attention_mask = batch['attention_mask'].to(cuda_device)\n",
    "        labels = batch['label'].to(cuda_device)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get predicted class (argmax of logits)\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Count correct predictions\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "# Calculate final accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Correct Predictions: {correct_predictions:,}/{total_predictions:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save the Trained Model\n",
    "\n",
    "### Why Save the Model?\n",
    "After training, we save the model weights so we can:\n",
    "- Use the model later without retraining\n",
    "- Deploy the model to production\n",
    "- Share the model with others\n",
    "- Resume training from a checkpoint\n",
    "\n",
    "### How to Load Later:\n",
    "```python\n",
    "model = ModernBertForSequenceClassification.from_pretrained(\"answerdotai/ModernBERT-base\", num_labels=6)\n",
    "model.load_state_dict(torch.load('emotion_classifier_model.pth'))\n",
    "model.eval()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's state dictionary (all weights and biases)\n",
    "# This creates a file that can be loaded later for inference or continued training\n",
    "torch.save(model.state_dict(), 'emotion_classifier_model.pth')\n",
    "\n",
    "print(\"\u2713 Model saved successfully to 'emotion_classifier_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Results Summary and Interpretation\n",
    "\n",
    "### Training Performance\n",
    "Based on the training output, our model shows excellent learning:\n",
    "\n",
    "**Loss Progression:**\n",
    "- **Epoch 1**: Average Loss = ~0.42\n",
    "- **Epoch 2**: Average Loss = ~0.12 (71% reduction)\n",
    "- **Epoch 3-5**: Continued improvement\n",
    "\n",
    "The decreasing loss indicates the model is successfully learning to classify emotions.\n",
    "\n",
    "### Validation Performance\n",
    "**Expected Accuracy: 94%+** \ud83c\udf89\n",
    "\n",
    "This is an excellent result! The model correctly predicts emotions over 94% of the time on unseen data.\n",
    "\n",
    "### Confusion Matrix Analysis\n",
    "\n",
    "**Strong Performance (High Diagonal Values):**\n",
    "- The model has high accuracy across all emotion categories\n",
    "- Diagonal values close to 0.9+ indicate reliable predictions\n",
    "\n",
    "**Common Confusions:**\n",
    "- **Sadness vs Fear**: Sometimes confused as both are negative emotions\n",
    "- **Joy vs Love**: Both are positive emotions with similar linguistic patterns\n",
    "- **Anger vs Sadness**: Can overlap when expressing frustration\n",
    "\n",
    "### Why This Model Works Well:\n",
    "\n",
    "1. **Pre-trained Knowledge**: ModernBERT was pre-trained on massive text corpora\n",
    "2. **Fine-tuning**: We adapted the model specifically for emotion classification\n",
    "3. **Sufficient Data**: 16,000 training examples provide good coverage\n",
    "4. **Appropriate Hyperparameters**: Learning rate and weight decay were well-tuned\n",
    "\n",
    "### Model Limitations:\n",
    "\n",
    "1. **Subtle Emotions**: May struggle with sarcasm or mixed emotions\n",
    "2. **Context**: Limited to 300 tokens, may miss broader context\n",
    "3. **Cultural Differences**: Training data may have cultural biases\n",
    "4. **Ambiguous Cases**: Some texts genuinely express multiple emotions\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**To Improve Further:**\n",
    "- Train for more epochs (monitor for overfitting)\n",
    "- Try different learning rates\n",
    "- Use data augmentation\n",
    "- Ensemble multiple models\n",
    "\n",
    "**To Deploy:**\n",
    "- Export model to ONNX format for faster inference\n",
    "- Create a REST API endpoint\n",
    "- Add confidence thresholds for uncertain predictions\n",
    "\n",
    "**To Analyze:**\n",
    "- Test on the held-out test set\n",
    "- Calculate per-class precision, recall, and F1 scores\n",
    "- Analyze misclassified examples manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Using the Model for Inference\n",
    "\n",
    "Now that our model is trained, let's see how to use it to predict emotions in new text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_emotion(text):\n",
    "    \"\"\"\n",
    "    Predict the emotion for a given text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predicted_emotion, confidence, all_probabilities)\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=300, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to GPU\n",
    "    input_ids = inputs['input_ids'].to(cuda_device)\n",
    "    attention_mask = inputs['attention_mask'].to(cuda_device)\n",
    "    \n",
    "    # Get model predictions (no gradient needed)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to probabilities using softmax\n",
    "        probabilities = F.softmax(logits, dim=1)[0]\n",
    "        \n",
    "        # Get the predicted class and confidence\n",
    "        confidence, predicted_class = torch.max(probabilities, dim=0)\n",
    "        \n",
    "    # Map prediction to emotion label\n",
    "    predicted_emotion = label_mapping[predicted_class.item()]\n",
    "    \n",
    "    return predicted_emotion, confidence.item(), probabilities.cpu().numpy()\n",
    "\n",
    "\n",
    "# Test with example sentences\n",
    "test_sentences = [\n",
    "    \"I'm so happy and excited about my new job!\",\n",
    "    \"This is the worst day ever, I can't believe this happened.\",\n",
    "    \"I'm really scared about the exam tomorrow.\",\n",
    "    \"I absolutely love spending time with my family!\",\n",
    "    \"I can't believe you did that, I'm furious!\",\n",
    "    \"Wow, I didn't expect that at all!\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EMOTION PREDICTION EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for text in test_sentences:\n",
    "    emotion, confidence, probs = predict_emotion(text)\n",
    "    print(f\"\\nText: \\\"{text}\\\"\")\n",
    "    print(f\"Predicted Emotion: {emotion.upper()}\")\n",
    "    print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "    print(f\"All probabilities:\")\n",
    "    for i, prob in enumerate(probs):\n",
    "        print(f\"  {label_mapping[i]:10s}: {prob*100:.2f}%\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccf Analyzing Maximum Sequence Length\n",
    "\n",
    "This analysis helps us understand why we chose `max_length=300` for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the longest text in the training dataset\n",
    "# This helps determine the optimal max_length for tokenization\n",
    "longest_text = max(train_ds['text'], key=len)\n",
    "\n",
    "# Print the length of the longest text\n",
    "print(f\"Maximum text length: {len(longest_text)} characters\")\n",
    "print(f\"\\nLongest text sample (first 200 chars):\")\n",
    "print(f\"\\\"{longest_text[:200]}...\\\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}